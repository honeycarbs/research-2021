\chapter[Определение тональности на основе векторного анализа]{Определение тональности\\на основе векторного\\анализа}
В данном разделе указан общий алгоритм определения тональности на основе векторного анализа и дан краткий обзор некоторым методам и математическим моделям.
\section{Общее описание подхода}
Как было указано ранее, для реализации подхода, основанного на векторном анализе требуется отобрать некоторое количество текстов на основе которых обучается классификатор. Текстам вручную расставляется отметка тональности, после этого выбирается алгоритм классификации. 
Алгоритм создания системы анализа тональности текста на основе векторного анализа приведен ниже.
\begin{enumerate}
	\item Выбор алгоритма предобработки текста.
	\item Выбор алгоритма классификации.
	\item Выбор признаков представления текста.
	\item Выбор способа подсчета веса признака.
\end{enumerate}

Некоторые этапы алгоритма требуют более детального рассмотрения. 
\section{Предобработка текста}
Перед обучением классификатора следует осуществить предобработку текстовой информации. Например, в работе \cite{prepr} использовалась леммизация --- все слова преобразовывались к словарной форме (лемме) при помощи морфологического анализатора mystem\cite{mystem} от компании Яндекс. Из текстов исключались англоязычные и русскоязычные <<стоп-слова>> (частицы, предлоги, местоимения), удалялись слова длиной менее трех
символов.
\section{Некоторые алгоритмы классификации}
Одними из самых широко используемых алгоритмов классификации являются:
\begin{itemize}
	\item SVM (\textit{Support Vector Machines}) --- алгоритм, создающий линейную гиперплоскость, которая разделяет данные на классы;
	\item наивный байесовский классификатор --- вероятностный классификатор, основанный на применении теоремы Байеса;
	\item метод на основе ключевых слов --- применение лексико --- статического анализа и составление списка ключевых слов для каждого класса. 
\end{itemize}
\section{Признаки представления текста}
Существует множество подходов представления текста.

Метод <<мешок слов>> представляет собой представление текста в виде вектора слов, встречающихся в тексте. В модели \cite{bw1} нейронная сеть строит векторное пространство слов, над которыми производятся векторные операции, включая определение мер близости или семантически значимые операции вычитания и сложения (Например, \textit{<<принц>> - мужчина + женщина = <<принцесса>>}).

В работе \cite{bw2} описан иной подход на основе кластеризации bag-of-words векторов лексических подстановок, сгенерированных нейросетевыми языковыми моделями. Идея метода состоит в следующем: интересующее слово в целевом корпусе заменяется токеном или последовательностью токенов, и в таком виде подается предсказателю, который  предсказывает возможные замены для токена в этом контексте. 

Часто при выборе представления текста учитываются части речи и пунктуация(например, количество восклицательных или вопросительных знаков).

\section{Способ подсчета веса признака}
Как и в случае оценочных словарей, существует множество подходов к подсчету веса признака. Некоторые модели ограничиваются <<булевой>> формой (вес есть --- веса нет), а некоторые вводят шкалу взвешивания и вводят частотность.

Наиболее часто используемый подход взвешивания --- \textit{tf.idf}\cite{tf}. Tf.idf (term frequency-inverse document frequency) является популярной метрикой при решении задач информационного поиска и анализа текста. Tf.idf представляет собой статистическую меру того, насколько термин важен в документе, который является частью корпуса. С использованием Tf.idf важность термина пропорциональна количеству встречаемости термина в документе и обратно пропорциональна количеству встречаемости термина во всей
коллекции документов.
У данной модели есть улучшения --- в работе \cite{delta} представлен подход delta TFIDF. Основное отличие в том, что учитывается, насколько неравномерно распределено слово в двух классах тональности (положительный или отрицательный). Формула для определения веса имеет следующий вид: 
\begin{equation}
	weight_i = tf_i \cdot \log_2\left(\cfrac{N_1 \cdot df_{i, 2}}{N_2 \cdot df_{i, 1}}\right),
\end{equation} 
где $N_1$ --- количество документов в положительном классе, $N_2$ --- в отрицательном, $df_{i, 1}$ --- количество документов положительного класса, в которых встречалось слово, $df_{i, 2}$ --- соответственно, в отрицательном, $tf$ --- частота вхождения, которая вычисляется согласно следующей формуле:
\begin{equation}
	tf(t, d) = \cfrac{n_t}{\sum_{k}n_k},
\end{equation}
$n_t$ --- число вхождений исследуемого слова в данный документ, а в знаменателе --- общее число слов.

Главное преимущество методов, использующих машинное обучение --- это высокие показатели эффективности. Как правило, если при обучении классификатора используется достаточно обширная размеченная подборка текстов, то метод показывает большую эффективность, чем шаблонный метод. Однако, метод имеет ряд недостатков. 

Во --- первых, создание обучающей коллекции --- это трудоемкий процесс. Во --- вторых, алгоритмы плохо переносятся на другую предметную область, поскольку это требует перенастройки системы \cite{rework}. 
\section{Вывод}
Методы, основанные на векторном анализе, показывают высокую эффективность работы. Однако, построенная система <<привязана>> к конкретной предметной области. В то же время, для работы данного метода необязательно заранее знать сущности, с которыми будет проведена работа. Соответственно, метод применим для текстов любой тематики и направленности, но ограниченных одной предметной областью.